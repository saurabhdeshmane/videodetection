# -*- coding: utf-8 -*-
"""Copy of Video_Cam_Detection_eclerx.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZOpVK54s3oPVX-foeCB9vRwzv0SOllt

***Face_Recognition Library***
* **Face detection:** face_recognition can locate faces in images and return the
bounding box coordinates of each detected face.
* **Face recognition:**The library allows you to compare faces and determine whether they belong to the same person.
* **Face landmarks:** It can identify facial landmarks (e.g., eyes, nose, mouth) in images.
* **Face encoding:** The library provides a way to encode facial features into a numerical representation, which is useful for face comparison.
"""

!pip install face_recognition

from google.colab import drive
drive.mount('/content/drive')

"""Printing Properties of Sample Video"""

import cv2

# Define video file path
video_file_path = "/content/drive/MyDrive/eclerx_video_processig/sample_zoom_rec.mp4"

# Capture video
cap = cv2.VideoCapture(video_file_path)

# Check if video is opened successfully
if not cap.isOpened():
    print("Error opening video file")
    exit(1)

# Get video properties
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)
duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / fps

# Print video properties
print("Resolution:", f"{width}x{height}")
print("Frame Rate:", f"{fps:.2f} FPS")
print("Duration:", f"{duration:.2f} seconds")

# Release video capture
cap.release()

"""Face Detection and Tracking using Face_Recognition Library (Original FPS)"""

import cv2
from google.colab.patches import cv2_imshow
import face_recognition
import numpy as np
import pandas as pd
import random
import time

# Record the start time
start_time = time.time()

# Dictionary to store unique face samples and their appearance durations
unique_faces = {}

# Load the video file
cap = cv2.VideoCapture('/content/drive/MyDrive/eclerx_video_processig/sample_zoom_rec.mp4')

# Initialize variables
face_locations = []
face_encodings = []
current_faces = []

# Define the fraction of frames to print output for (e.g., 1 in every N frames)
output_fraction = 10
frame_counter = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Find face locations and face encodings in the current frame
    current_faces = face_recognition.face_locations(frame)
    current_encodings = face_recognition.face_encodings(frame, current_faces)

    # Identify unique faces and track their appearance durations
    for face_location, face_encoding in zip(current_faces, current_encodings):
        matched_face_id = None

        # Check if there are existing faces
        if unique_faces:
            for stored_face_id, stored_face_data in unique_faces.items():
                # Compare with existing faces using a threshold
                match = face_recognition.compare_faces([stored_face_id], face_encoding, tolerance=0.6)[0]
                if match:
                    # Found a match, use the existing face ID
                    matched_face_id = stored_face_id
                    break

        if matched_face_id is None:
            # No match found, create a new face ID
            new_face_id = tuple(face_encoding)
            unique_faces[new_face_id] = {'start_frame': cap.get(cv2.CAP_PROP_POS_FRAMES), 'duration': 1,
                                          'sample_image': frame[face_location[0]:face_location[2], face_location[3]:face_location[1]]}
        else:
            # Existing face detected, update duration
            unique_faces[matched_face_id]['duration'] += 1

    # Draw rectangles around detected faces (optional)
    for (top, right, bottom, left) in current_faces:
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)

    # Display the frame only for a fraction of frames
    frame_counter += 1
    if frame_counter % output_fraction == 0:
        cv2_imshow(frame)

    if cv2.waitKey(1) & 0xFF == 27:  # Press 'Esc' to exit
        break

cap.release()
cv2.destroyAllWindows()

# Record the end time
end_time = time.time()

# Calculate and print the execution time
execution_time = end_time - start_time
print(f"Execution time: {execution_time} seconds")

# Create a DataFrame from the unique_faces dictionary
df = pd.DataFrame([(face_id, data['start_frame'], data['duration']) for face_id, data in unique_faces.items()],
                  columns=['Face_ID', 'Start_Frame', 'Duration'])

# Print or display the DataFrame
print(df)

# Print one random sample image for each unique face
for face_id, data in unique_faces.items():
    print(f"Unique face {face_id} - Sample Image:")
    cv2_imshow(data['sample_image'])  # Display only one sample image for illustration

"""Count of Face Detected Frames"""

df.head()

"""Time Duration in Seconds"""

df = df.rename(columns={'Duration': 'Count'})

# Create the 'Time_Duration' column
df['Time_Duration'] = df['Count'] / fps

df.head()

"""Reduce FPS 50%"""

import cv2
from google.colab.patches import cv2_imshow
import face_recognition
import numpy as np
import pandas as pd
import random
import time

# Record the start time
start_time = time.time()

# Dictionary to store unique face samples and their appearance durations
unique_faces = {}

# Load the video file
cap = cv2.VideoCapture('/content/drive/MyDrive/eclerx_video_processig/sample_zoom_rec.mp4')

# Initialize variables
face_locations = []
face_encodings = []
current_faces = []

# Define the fraction of frames to print output for (e.g., 1 in every N frames)
output_fraction = 10
frame_counter = 0

# Process every other frame
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Skip every other frame
    if frame_counter % 2 != 0:
        frame_counter += 1
        continue

    # Find face locations and face encodings in the current frame
    current_faces = face_recognition.face_locations(frame)
    current_encodings = face_recognition.face_encodings(frame, current_faces)

    # Identify unique faces and track their appearance durations
    for face_location, face_encoding in zip(current_faces, current_encodings):
        matched_face_id = None

        # Check if there are existing faces
        if unique_faces:
            for stored_face_id, stored_face_data in unique_faces.items():
                # Compare with existing faces using a threshold
                match = face_recognition.compare_faces([stored_face_id], face_encoding, tolerance=0.6)[0]
                if match:
                    # Found a match, use the existing face ID
                    matched_face_id = stored_face_id
                    break

        if matched_face_id is None:
            # No match found, create a new face ID
            new_face_id = tuple(face_encoding)
            unique_faces[new_face_id] = {'start_frame': cap.get(cv2.CAP_PROP_POS_FRAMES), 'duration': 1,
                                         'sample_image': frame[face_location[0]:face_location[2],
                                                        face_location[3]:face_location[1]]}
        else:
            # Existing face detected, update duration
            unique_faces[matched_face_id]['duration'] += 1

    # Draw rectangles around detected faces (optional)
    for (top, right, bottom, left) in current_faces:
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)

    # Display the frame only for a fraction of frames
    if frame_counter % (output_fraction * 2) == 0:
        cv2_imshow(frame)

    if cv2.waitKey(1) & 0xFF == 27:  # Press 'Esc' to exit
        break

    frame_counter += 1

cap.release()
cv2.destroyAllWindows()

# Record the end time
end_time = time.time()

# Calculate and print the execution time
execution_time = end_time - start_time
print(f"Execution time: {execution_time} seconds")

# Create a DataFrame from the unique_faces dictionary
df = pd.DataFrame([(face_id, data['start_frame'], data['duration']) for face_id, data in unique_faces.items()],
                  columns=['Face_ID', 'Start_Frame', 'Duration'])

# Print or display the DataFrame
print(df)

# Print one random sample image for each unique face
for face_id, data in unique_faces.items():
    print(f"Unique face {face_id} - Sample Image:")
    cv2_imshow(data['sample_image'])  # Display only one sample image for illustration

"""

*   Detecting Motion along with recognition

"""

import cv2
from google.colab.patches import cv2_imshow
import face_recognition
import numpy as np
import pandas as pd
import random
import time

# Function to calculate the absolute difference between two images
def image_diff(img1, img2):
    diff = cv2.absdiff(img1, img2)
    diff_gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
    _, threshold_diff = cv2.threshold(diff_gray, 30, 255, cv2.THRESH_BINARY)
    return np.sum(threshold_diff)

# Record the start time
start_time = time.time()

# Dictionary to store unique face samples and their appearance durations
unique_faces = {}

# Load the video file
cap = cv2.VideoCapture('/content/drive/MyDrive/eclerx_video_processig/sample_zoom_rec.mp4')

# Initialize variables
face_locations = []
face_encodings = []
current_faces = []

# Initialize motion detection variables
motion_frame_count = 0
consecutive_no_motion_frames = 100

# Define the fraction of frames to print output for (e.g., 1 in every N frames)
output_fraction = 10
frame_counter = 0

# Process every other frame
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Skip every other frame
    if frame_counter % 2 != 0:
        frame_counter += 1
        continue

    # Find face locations and face encodings in the current frame
    current_faces = face_recognition.face_locations(frame)
    current_encodings = face_recognition.face_encodings(frame, current_faces)

    # Detect motion
    if frame_counter > 0:
        prev_frame = cap.read()[1]
        motion_score = image_diff(frame, prev_frame)
        if motion_score > 1000:
            motion_frame_count = 0
        else:
            motion_frame_count += 1

    # If no motion detected for consecutive_no_motion_frames, skip processing
    if motion_frame_count >= consecutive_no_motion_frames:
        frame_counter += 1
        continue

    # Identify unique faces and track their appearance durations
    for face_location, face_encoding in zip(current_faces, current_encodings):
        matched_face_id = None

        # Check if there are existing faces
        if unique_faces:
            for stored_face_id, stored_face_data in unique_faces.items():
                # Compare with existing faces using a threshold
                match = face_recognition.compare_faces([stored_face_id], face_encoding, tolerance=0.6)[0]
                if match:
                    # Found a match, use the existing face ID
                    matched_face_id = stored_face_id
                    break

        if matched_face_id is None:
            # No match found, create a new face ID
            new_face_id = tuple(face_encoding)
            unique_faces[new_face_id] = {'start_frame': cap.get(cv2.CAP_PROP_POS_FRAMES), 'duration': 1,
                                         'sample_image': frame[face_location[0]:face_location[2],
                                                        face_location[3]:face_location[1]]}
        else:
            # Existing face detected, update duration
            unique_faces[matched_face_id]['duration'] += 1

    # Draw rectangles around detected faces (optional)
    for (top, right, bottom, left) in current_faces:
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)

    # Display the frame only for a fraction of frames
    if frame_counter % (output_fraction * 2) == 0:
        cv2_imshow(frame)

    if cv2.waitKey(1) & 0xFF == 27:  # Press 'Esc' to exit
        break

    frame_counter += 1

cap.release()
cv2.destroyAllWindows()

# Record the end time
end_time = time.time()

# Calculate and print the execution time
execution_time = end_time - start_time
print(f"Execution time: {execution_time} seconds")

# Create a DataFrame from the unique_faces dictionary
df = pd.DataFrame([(face_id, data['start_frame'], data['duration']) for face_id, data in unique_faces.items()],
                  columns=['Face_ID', 'Start_Frame', 'Duration'])

# Print or display the DataFrame
print(df)

# Print one random sample image for each unique face
for face_id, data in unique_faces.items():
    print(f"Unique face {face_id} - Sample Image:")
    cv2_imshow(data['sample_image'])  # Display only one sample image for illustration

import cv2
from google.colab.patches import cv2_imshow
import face_recognition
import numpy as np
import pandas as pd

# Dictionary to store unique face samples and their appearance durations
unique_faces = {}

# Load the video file
cap = cv2.VideoCapture('/content/drive/MyDrive/eclerx_video_processig/sample_zoom_rec.mp4')

# Initialize variables
face_locations = []
face_encodings = []
current_faces = []

# Define the fraction of frames to print output for (e.g., 1 in every N frames)
output_fraction = 10
frame_counter = 0

# Set a threshold for the minimum motion of the detected face
min_motion_threshold = 20  # Adjust as needed

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Find face locations and face encodings in the current frame
    current_faces = face_recognition.face_locations(frame)
    current_encodings = face_recognition.face_encodings(frame, current_faces)

    # Identify unique faces and track their appearance durations
    for face_location, face_encoding in zip(current_faces, current_encodings):
        # Calculate the motion of the detected face region
        if tuple(face_location) in unique_faces:
            motion = np.sum(np.abs(frame[face_location[0]:face_location[2], face_location[3]:face_location[1]] -
                                   unique_faces[tuple(face_location)]['sample_image']))
        else:
            motion = min_motion_threshold + 1

        if motion > min_motion_threshold:
            matched_face_id = None

            # Check if there are existing faces
            if unique_faces:
                for stored_face_id, stored_face_data in unique_faces.items():
                    # Compare with existing faces using a threshold
                    match = face_recognition.compare_faces([stored_face_id], face_encoding, tolerance=0.6)[0]
                    if match:
                        # Found a match, use the existing face ID
                        matched_face_id = stored_face_id
                        break

            if matched_face_id is None:
                # No match found, create a new face ID
                new_face_id = tuple(face_encoding)
                unique_faces[new_face_id] = {'start_frame': cap.get(cv2.CAP_PROP_POS_FRAMES), 'duration': 1,
                                              'sample_image': frame[face_location[0]:face_location[2], face_location[3]:face_location[1]]}
            else:
                # Existing face detected, update duration
                unique_faces[matched_face_id]['duration'] += 1

    # Draw rectangles around detected faces (optional)
    for (top, right, bottom, left) in current_faces:
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)

    # Display the frame only for a fraction of frames
    frame_counter += 1
    if frame_counter % output_fraction == 0:
        cv2_imshow(frame)

    if cv2.waitKey(1) & 0xFF == 27:  # Press 'Esc' to exit
        break

cap.release()
cv2.destroyAllWindows()

# Create a DataFrame from the unique_faces dictionary
df = pd.DataFrame([(face_id, data['start_frame'], data['duration']) for face_id, data in unique_faces.items()],
                  columns=['Face_ID', 'Start_Frame', 'Duration'])

# Print or display the DataFrame
print(df)

# Print one random sample image for each unique face
for face_id, data in unique_faces.items():
    print(f"Unique face {face_id} - Sample Image:")
    cv2_imshow(data['sample_image'])
    break  # Display only one sample image for illustration